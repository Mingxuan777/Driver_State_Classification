{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The state of drivers when driving is critical to making driving safe. Drinking and talking while driving are two major issues that have an impact on driving safety. As a result, creating a program that can automatically determine a driver’s status is critical. The State Farm Distracted Driver Detection Dataset includes nine driver statuses: safe driving, texting, chatting on the phone, ma- nipulating the radio, reaching behind the wheel, applying makeup, and conversing with a passenger.\n",
    "\n",
    "MobileNet, a deep neural network model, is utilized to identify photos in this research. This research also investigates how hyperparameters influence the training process and training accuracy by comparing the effects of hyperparameter change on experimental findings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Methods\n",
    "## Deep Leaning method: MobileNet\n",
    "To identify photos, a variety of deep learning approaches can be utilized. Lenet, VGG, and other convolutional neural network-based image categorization approaches are examples. In general, deep learning-based image classification technology uses a multi-layer convolution module to extract features from images, followed by a fully connected layer to turn the acquired features into a one-dimensional vector.After that, several fully connected layers are followed and eventually output the possibility of each classification label. The number of final classifications is equal to the number of neurons in the output layer.\n",
    "\n",
    "MobileNet[1] is a light-weight deep neural network model used in this research that uses depth-wise separable convolutions instead of typical convolutions. MobileNet's parameter number is substantially lower than that of and the network is also faster to train thanks to deep-wise separable convolutions. Accordingly, MobileNet is suitable to be deployed on mobile devices.\n",
    "\n",
    "A 3x3 convolutional layer, a batch normalization layer, and a ReLU activation layer make up a standard convolutional layer. The 3x3 convolutional layer is replaced by a 3x3 Depthwise convolutional layer in the depth-wise separable convolutional layer. Following that, there is a 1x1 convolutional layer with a batch normalization layer and a ReLU layer. The comparison between the two architectures is depicted in the diagram below[1]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](pics/COMP.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "MobileNet performs picture feature extraction using a combination of deep-wise convolutional layers and normal convolutional layers. MobileNet with depth-wise separable convoutional layers performs somewhat worse than Conv MobileNet (MobileNet built with full convolutions) (71.7 percent and 70.6 percent on ImageNet). However, the latter has a far fewer number of parameters (29.3 millions versus 4.2 millions)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](pics/COMP.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment Process\n",
    "This project consists of three files: load data.py, main.py, and VGG.py. The first method is for reading raw picture data into tensor. When it comes to training data, the RandomHorizontalFlip technique is utilized to horizontally flip training images. With the use of parameter traindata_size and valdata_size, load data.py separates training and validation data and makes the size controllable . Special methods are used in the design of load data.py to make the function more efficient. The parameter setting function, as well as the training and validation programs, are all included in main.py. To acquire the performance of the present model parameters, a validation set is implemented immediately after training. The model is saved as model.pt once each epoch is completed. VGG.py contains the neural network models VGG16 and VGG19, which I created myself."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters comparison\n",
    "This section reports experiment result. There are six experiments in total, and in each of them, one parameter is modified. The first experiment group is the standard group, which can be regarded as the baseline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standard Group\n",
    "This experiment group set epoch number as **50**, learning rate as **0.0001**, batch size as **16** and dataset parameters as **0.6 and 0.95**. Following is the training process:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](pics/SG_1.png)\n",
    "![](pics/SG_2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The loss value starts to drop from the beginning of training and stabilizes at the global step of 25k, as shown in the graphs above. After a while, accuracy hits around 0.97, and loss drops to approximately 0.2 to 0.4."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Different Epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](pics/Epoch_1.png)\n",
    "![](pics/Epoch_2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In comparison to the standard group, the preceding figures show that, while the loss value is nearly stable, accuracy is not. In the end, it hovers around 0.95, which is somewhat lower than the typical group's accuracy rate. Finally, sufficient training epoches are required to ensure that the neural network is properly trained and that its best performance is demonstrated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hide_input": true
   },
   "source": [
    "### Different Learning Rate\n",
    "\n",
    "This experiment group set epoch number as **50**, learning rate as **0.001**, batch size as **16** and dataset parameters as **0.6 and 0.95**. Following is the training process:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](pics/LR_1.png)\n",
    "![](pics/LR_2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The learning rate is changed from 0.0001 to 0.001 in this experiment. When it comes to the loss graph, it's worth noting that the loss curve sways a lot during training. It fluctuates between 0.4 and 0.8 after 50 epoches of training, compared to 0.4 to 0.6 in the control group. This experiment group's accuracy (about 95%) is also lower than the standard group's (around 96 percent)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Different Batch Size\n",
    "\n",
    "This experiment group set epoch number as **50**, learning rate as **0.0001**, batch size as **8** and dataset parameters as **0.6 and 0.95**. Following is the training process:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](pics/BS_1.png)\n",
    "![](pics/BS_2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Batch size is set to 8 instead of 16 in this experiment group. As a result, the worldwide step count has risen to 100,000. Meanwhile, it's worth noting that the training period will be longer and the loss curve appears to oscillate more than the standard group's."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### SGD over Adam\n",
    "\n",
    "This experiment group set epoch number as **50**, learning rate as **0.0001**, batch size as **16** and dataset parameters as **0.6 and 0.95**. However, the optimizer uses SGD rather than Adam. Following is the training process:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](pics/SGD_1.png)\n",
    "![](pics/SGD_2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This study group compares the effects of several optimizers on the training process. When compared to the Adam optimizer, the loss in SGD decays far more slowly. It only starts to decay at a global step of roughly 10k, owing to the fact that the SGD optimizer is more likely to fall into a local minimum. The accuracy curve follows a similar pattern. After the 13th global step, it only starts to increase. The accuracy eventually arrives at 90%, which is lower than the standard group. Adam is a superior Optimizer than SGD, according to the trial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Different Training Batch Size\n",
    "\n",
    "This experiment group set epoch number as **50**, learning rate as **0.0001**, batch size as **16** and dataset parameters as **0.6 and 0.95**. Following is the training process:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"pics/Training_1.png\" width=\"80%\">\n",
    "<img src=\"pics/Training_2.png\" width=\"80%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "The chart above shows how different hyperparameters affect the training process. Overall, epoch refers to the overall number of training iterations, and having a suﬀicient number of training iterations is crucial for excellent training accuracy. Because the learning rate determines how quickly the gradient lowers, a low learning rate usually results in a slower training process, whereas a high learning rate produces a lot of oscillation as the loss decreases. When it comes to batch size, a lower batch size lengthens the training process overall, but the end accuracy remains the same. The choice of optimizer, according to the preceding trials, is also crucial in achieving good training accuracy. Adam outperforms SGD significantly. The Adam technique can help the training process get to the fitting state faster and cross the local minimum value more effectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion and Discussion\n",
    "This project uses MobileNet to classify driver images. Meanwhile, it investigates how hyperparam- eter settings affect the training process. According to the previous analysis, all five parameters have a direct influence on the training process, with the exception of batch size, which only influences the overall global step size and then the total training duration.\n",
    "Despite the foregoing findings, a deeper investigation into parameter tweaking is still required. Despite the fact that a lower learning rate means a lower oscillation, a low learning rate slows down the training process. The employment of a dynamic learning rate is a frequent solution to this problem. This approach starts with a high learning rate, but it gradually decreases after each epoch. Both the training time and the oscillation issue are balanced as a result. The batch size is in a similar scenario. If the training is done on GPU, a large batch size necessitates a large graphic memory, and if the dataset is too huge, it may become a problem. Another element that must be carefully studied is the epoch number. The neural network model may not be fully trained if the epoch number is too small. However, if it becomes too large, the additional training time will become unnecessary. Finally, selecting an optimizer is critical. As can be seen in the prior analysis, Adam is a newer and superior optimizer than SGD.\n",
    "In the furture, a more detailed comparision should be considered. For example, a test of whether the aforementioned dynamic learning rate method improves the overall training process in the sense of accuracy and training duation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hide_input": false
   },
   "source": [
    "# Reference\n",
    "[1] Howard, A.G., Zhu, M., Chen, B., Kalenichenko, D., Wang, W., Weyand, T., Andreetto, M. and Adam, H., 2017. Mobilenets: Eﬀicient convolutional neural networks for mobile vision applications. arXiv preprint arXiv:1704.04861.Howard, A.G., Zhu, M., Chen, B., Kalenichenko, D., Wang, W., Weyand, T., Andreetto, M. and Adam, H., 2017. Mobilenets: Eﬀicient convolutional neural networks for mobile vision applications. arXiv preprint arXiv:1704.04861."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
